{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(656292, 14)\n",
      "None\n",
      "INFO:try_feWavLM_MT2:{'train': {'use_cuda': True, 'log_interval': 20, 'seed': 1234, 'epochs': 10000, 'learning_rate': 0.0005, 'betas': [0.8, 0.99], 'eps': 1e-09, 'lr_decay': 0.999875, 'warmup_steps': 0, 'scheduler': 'noam', 'batch_size': 256, 'accumulation_steps': 1, 'fp16_run': False, 'warm_start': False, 'loss_function': 'CE', 'from_pretrain': False, 'warm_start_checkpoint_pool': './logs/Dim_PoolingSep_VATTTry6_Roberto_normmax/best_pool.pth', 'ignored_layer': []}, 'data': {'max_value_norm': False, 'max_wav_value': 32768.0, 'sampling_rate': 16000, 'filter_length': 1024, 'hop_length': 256, 'win_length': 1024, 'n_mel_channels': 80, 'mel_fmin': 0.0, 'mel_fmax': 8000.0, 'desired_length': 1.2, 'fade_samples_ratio': 16, 'pad_types': 'zero', 'acoustic_feature': False, 'feature_type': 'melspectogram', 'multimask_augment': False, 'augment_data': False, 'add_noise': False, 'mix_audio': False, 'many_class': 2, 'db_path': '/run/media/fourier/Data1/Pras/Database_ThesisNew/', 'metadata_csv': 'TB/tb_metadata_all_cleaned.csv'}, 'model': {'pooling_model': 'WavLM_MyOwn2MT', 'feature_dim': 80, 'p_dropout': 0.1, 'regress_hidden_dim': 1024, 'regress_dropout': 0.1, 'regress_layers': 1, 'output_dim': 2}, 'model_dir': './logs/try_feWavLM_MT2'}\n",
      "(1, 19200)\n",
      "INFO:try_feWavLM_MT2:======================================\n",
      "INFO:try_feWavLM_MT2:✨ Loss: CE\n",
      "INFO:try_feWavLM_MT2:✨ Use Between Class Training: False\n",
      "INFO:try_feWavLM_MT2:✨ Use Augment: False\n",
      "INFO:try_feWavLM_MT2:✨ Padding Type: zero\n",
      "INFO:try_feWavLM_MT2:✨ Using Model: WavLM_MyOwn2MT\n",
      "INFO:try_feWavLM_MT2:======================================\n"
     ]
    }
   ],
   "source": [
    "import os, json, pickle, inspect\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.amp import GradScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoModel, AutoConfig, AutoFeatureExtractor\n",
    "\n",
    "import utils\n",
    "import commons\n",
    "import models\n",
    "from cough_datasets import MTCoughDatasets, MTCoughDatasetsCollate, CoughDatasets, CoughDatasetsCollate\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, confusion_matrix, roc_curve, auc, roc_auc_score, f1_score\n",
    "\n",
    "from tensorboard.backend.event_processing import event_accumulator\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "\n",
    "# =============================================================\n",
    "# SECTION: Intialize Data\n",
    "# =============================================================\n",
    "INIT = False\n",
    "MODEL_NAME = \"try_feWavLM_MT2\"\n",
    "CONFIG_PATH = \"configs/lstm_cnn.json\"\n",
    "\n",
    "model_dir = os.path.join(\"./logs\", MODEL_NAME)\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "\n",
    "config_save_path = os.path.join(model_dir, \"config.json\")\n",
    "if INIT:\n",
    "    with open(CONFIG_PATH, \"r\") as f:\n",
    "      data = f.read()\n",
    "    with open(config_save_path, \"w\") as f:\n",
    "      f.write(data)\n",
    "else:\n",
    "    with open(config_save_path, \"r\") as f:\n",
    "      data = f.read()\n",
    "\n",
    "config = json.loads(data)\n",
    "  \n",
    "hps = utils.HParams(**config)\n",
    "hps.model_dir = model_dir\n",
    "\n",
    "BATCH_SIZE = hps.train.batch_size\n",
    "ACCUMULATION_STEP = hps.train.accumulation_steps\n",
    "assert (ACCUMULATION_STEP > 0) and (BATCH_SIZE % ACCUMULATION_STEP == 0)\n",
    "cur_bs = BATCH_SIZE // ACCUMULATION_STEP\n",
    "\n",
    "# =============================================================\n",
    "# SECTION: Loading Data\n",
    "# =============================================================\n",
    "\n",
    "##### Label Umum Semua\n",
    "Diseases_codes = [0, 1]\n",
    "CLASS_NAMES = [\"Healthy\", \"TB\"]\n",
    "\n",
    "df = pd.read_csv(f'{hps.data.db_path}/{hps.data.metadata_csv}')\n",
    "df = df[df['cough_score'] >= 0.90].sample(frac=1, random_state=40)\n",
    "\n",
    "df_solic = df[df['type_cough'] == 0].sample(frac=1, random_state=41)\n",
    "df_long = df[df['type_cough'] == 1].sample(frac=1, random_state=42) # 0 Solic, 1 Longi\n",
    "df_long_array = []\n",
    "for i_rand in range(5):\n",
    "    df_0 = df_long[df_long['disease_label'] == 0].sample(n=df_solic['disease_label'].value_counts()[0], random_state=i_rand * 4)\n",
    "    df_1 = df_long[df_long['disease_label'] == 1].sample(n=df_solic['disease_label'].value_counts()[1], random_state=i_rand * 4)\n",
    "    df_long_array.append(pd.concat([df_0, df_1], ignore_index=True, sort=False))\n",
    "\n",
    "df = df\n",
    "#df = df_solic\n",
    "#df = df_long_array[0]\n",
    "#df = df_long\n",
    "print(df.shape)\n",
    "\n",
    "df_train, df_test = train_test_split(df, test_size=0.1, random_state=42, shuffle=True)\n",
    "#df_train, df_test = df_long, df_solic\n",
    "\n",
    "class_frequencies = df_train['disease_label'].value_counts().to_dict()\n",
    "total_samples = len(df_train)\n",
    "class_weights = {cls: total_samples / (len(Diseases_codes) * freq) if freq != 0 else 0 for cls, freq in class_frequencies.items()}\n",
    "weights_list = [class_weights[cls] for cls in Diseases_codes]\n",
    "class_weights_tensor = torch.tensor(weights_list, device='cuda', dtype=torch.float)\n",
    "class_weights_tensor = None\n",
    "print(class_weights_tensor)\n",
    "\n",
    "# =============================================================\n",
    "# SECTION: Setup Logger, Dataloader\n",
    "# =============================================================\n",
    "logger = utils.get_logger(hps.model_dir)\n",
    "logger.info(hps)\n",
    "\n",
    "writer = SummaryWriter(log_dir=hps.model_dir)\n",
    "writer_eval = SummaryWriter(log_dir=os.path.join(hps.model_dir, \"eval\"))\n",
    "\n",
    "collate_fn = MTCoughDatasetsCollate(hps.data.many_class)\n",
    "train_dataset = MTCoughDatasets(df_train.values, hps.data, train=True)\n",
    "val_dataset = MTCoughDatasets(df_test.values, hps.data, train=False)\n",
    "\n",
    "#train_sampler = DistributedBucketSampler(train_dataset, cur_bs, [32,300,400,500,600,700,800,900,1000], num_replicas=1, rank=0, shuffle=True)\n",
    "#train_loader = DataLoader(train_dataset, num_workers=28, shuffle=False, pin_memory=True, collate_fn=collate_fn, batch_sampler=train_sampler)\n",
    "train_loader = DataLoader(train_dataset, num_workers=28, shuffle=True, batch_size=cur_bs, pin_memory=True, drop_last=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, num_workers=28, shuffle=False, batch_size=hps.train.batch_size, pin_memory=True, drop_last=True, collate_fn=collate_fn)\n",
    "\n",
    "print(next(iter(train_loader))[1][0].numpy().shape)\n",
    "# =============================================================\n",
    "# SECTION: Setup Logger, Dataloader\n",
    "# =============================================================\n",
    "logger.info(f\"======================================\")\n",
    "logger.info(f\"✨ Loss: {hps.train.loss_function}\")\n",
    "logger.info(f\"✨ Use Between Class Training: {hps.data.mix_audio}\")\n",
    "logger.info(f\"✨ Use Augment: {hps.data.augment_data}\")\n",
    "logger.info(f\"✨ Padding Type: {hps.data.pad_types}\")\n",
    "logger.info(f\"✨ Using Model: {hps.model.pooling_model}\")\n",
    "logger.info(f\"======================================\")\n",
    "\n",
    "epoch_str = 1\n",
    "global_step = 0\n",
    "\n",
    "pool_net = getattr(models, hps.model.pooling_model)\n",
    "pool_model = pool_net(hps.model.feature_dim, **hps.model).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:try_feWavLM_MT2:Loaded checkpoint './logs/try_feWavLM_MT2/best_pool.pth' (iteration 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2307 [00:02<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "_, _, _, _, epoch_str = utils.load_checkpoint(\n",
    "    os.path.join(hps.model_dir, \"best_pool.pth\"),\n",
    "    pool_model,\n",
    "    None,\n",
    "    None,\n",
    ")\n",
    "\n",
    "pool_model.eval() \n",
    "all_preds, all_labels, all_probs  = [], [], []\n",
    "\n",
    "# Classification (BCE or CE)\n",
    "all_smoker_preds, all_smoker_labels = [], []\n",
    "all_hemoptysis_preds, all_hemoptysis_labels = [], []\n",
    "all_gender_preds, all_gender_labels = [], []\n",
    "all_spk_preds, all_spk_labels = [], []\n",
    "\n",
    "# Regression\n",
    "all_age_preds, all_age_labels = [], []\n",
    "all_coughdur_preds, all_coughdur_labels = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (wav_name, wav_padded, attention_masks, dse_ids, smokers, hemoptysis, gender_ids, spk_ids, \n",
    "                    ages, cough_durs, tb_priors, tb_prior_Puls, tb_prior_Extrapuls, tb_prior_Unknowns) in enumerate(tqdm(train_loader)):\n",
    "        wav_padded = wav_padded.cuda(non_blocking=True).float().squeeze(1)\n",
    "        attention_masks = attention_masks.cuda(non_blocking=True).float()\n",
    "        dse_ids = dse_ids.cuda(non_blocking=True).long()\n",
    "        smokers = smokers.cuda(non_blocking=True).long()\n",
    "        hemoptysis = hemoptysis.cuda(non_blocking=True).long()\n",
    "        gender_ids = gender_ids.cuda(non_blocking=True).long()\n",
    "        spk_ids = spk_ids.cuda(non_blocking=True).long()\n",
    "        ages = ages.cuda(non_blocking=True).float()\n",
    "        cough_durs = cough_durs.cuda(non_blocking=True).float()\n",
    "        tb_priors = tb_priors.cuda(non_blocking=True).long()\n",
    "        tb_prior_Puls = tb_prior_Puls.cuda(non_blocking=True).long()\n",
    "        tb_prior_Extrapuls = tb_prior_Extrapuls.cuda(non_blocking=True).long()\n",
    "        tb_prior_Unknowns = tb_prior_Unknowns.cuda(non_blocking=True).long()\n",
    "        \n",
    "        x_lengths = torch.tensor(commons.compute_length_from_mask(attention_masks)).cuda(non_blocking=True).long()\n",
    "        out_model = pool_model(wav_padded)\n",
    "        outputs = out_model[0]\n",
    "        \n",
    "        probs = torch.softmax(outputs[\"dse\"], dim=1)\n",
    "        preds = torch.argmax(outputs[\"dse\"], dim=1)\n",
    "\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(dse_ids.cpu().numpy())\n",
    "        all_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "        # --- Binary classification (BCE)\n",
    "        all_smoker_preds.extend(torch.round(torch.sigmoid(outputs[\"smoker\"])).cpu().numpy())\n",
    "        all_smoker_labels.extend(smokers.cpu().numpy())\n",
    "\n",
    "        all_hemoptysis_preds.extend(torch.round(torch.sigmoid(outputs[\"hemoptysis\"])).cpu().numpy())\n",
    "        all_hemoptysis_labels.extend(hemoptysis.cpu().numpy())\n",
    "\n",
    "        all_gender_preds.extend(torch.round(torch.sigmoid(outputs[\"gender\"])).cpu().numpy())\n",
    "        all_gender_labels.extend(gender_ids.cpu().numpy())\n",
    "\n",
    "        # --- Spk: multi-class\n",
    "        all_spk_preds.extend(torch.argmax(outputs[\"spk\"], dim=1).cpu().numpy())\n",
    "        all_spk_labels.extend(spk_ids.cpu().numpy())\n",
    "\n",
    "        # --- Regression\n",
    "        all_age_preds.extend(outputs[\"age\"].squeeze().cpu().numpy())\n",
    "        all_age_labels.extend(ages.cpu().numpy())\n",
    "\n",
    "        all_coughdur_preds.extend(outputs[\"cough_dur\"].squeeze().cpu().numpy())\n",
    "        all_coughdur_labels.extend(cough_durs.cpu().numpy())\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "import os, json, pickle, inspect\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.amp import GradScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoModel, AutoConfig, AutoFeatureExtractor\n",
    "\n",
    "import utils\n",
    "import commons\n",
    "import models\n",
    "from cough_datasets import CoughDatasets, CoughDatasetsCollate\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, confusion_matrix, roc_curve, auc, roc_auc_score, f1_score\n",
    "\n",
    "from tensorboard.backend.event_processing import event_accumulator\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "\n",
    "# =============================================================\n",
    "# SECTION: Intialize Data\n",
    "# =============================================================\n",
    "INIT = True\n",
    "MODEL_NAME = \"try_faetureencwhisper\"\n",
    "CONFIG_PATH = \"configs/lstm_cnn.json\"\n",
    "\n",
    "model_dir = os.path.join(\"./logs\", MODEL_NAME)\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "\n",
    "config_save_path = os.path.join(model_dir, \"config.json\")\n",
    "if INIT:\n",
    "    with open(CONFIG_PATH, \"r\") as f:\n",
    "      data = f.read()\n",
    "    with open(config_save_path, \"w\") as f:\n",
    "      f.write(data)\n",
    "else:\n",
    "    with open(config_save_path, \"r\") as f:\n",
    "      data = f.read()\n",
    "\n",
    "config = json.loads(data)\n",
    "  \n",
    "hps = utils.HParams(**config)\n",
    "hps.model_dir = model_dir\n",
    "\n",
    "BATCH_SIZE = hps.train.batch_size\n",
    "ACCUMULATION_STEP = hps.train.accumulation_steps\n",
    "assert (ACCUMULATION_STEP > 0) and (BATCH_SIZE % ACCUMULATION_STEP == 0)\n",
    "cur_bs = BATCH_SIZE // ACCUMULATION_STEP\n",
    "\n",
    "# =============================================================\n",
    "# SECTION: Loading Data\n",
    "# =============================================================\n",
    "\n",
    "##### Label Umum Semua\n",
    "Diseases_codes = [0, 1]\n",
    "CLASS_NAMES = [\"Negative TB\", \"Positive TB\"]\n",
    "\n",
    "df = pd.read_csv(f'{hps.data.db_path}/GoogleHealth/google_tb_metadata.csv')\n",
    "df['path_file'] = f'GoogleHealth/' + df['path_file'] \n",
    "\n",
    "collate_fn = CoughDatasetsCollate(hps.data.many_class)\n",
    "val_dataset = CoughDatasets(df.values, hps.data, train=False)\n",
    "val_loader = DataLoader(val_dataset, num_workers=28, shuffle=False, batch_size=hps.train.batch_size, pin_memory=True, drop_last=True, collate_fn=collate_fn)\n",
    "# =============================================================\n",
    "# SECTION: Setup Logger, Dataloader\n",
    "# =============================================================\n",
    "epoch_str = 1\n",
    "global_step = 0\n",
    "\n",
    "pool_net = getattr(models, hps.model.pooling_model)\n",
    "pool_model = pool_net(hps.model.feature_dim, **hps.model).cuda()\n",
    "\n",
    "optimizer_p = torch.optim.AdamW(pool_model.parameters(), hps.train.learning_rate, betas=hps.train.betas, eps=hps.train.eps)\n",
    "scheduler_p = torch.optim.lr_scheduler.ExponentialLR(optimizer_p, gamma=hps.train.lr_decay, last_epoch=epoch_str - 2)\n",
    "\n",
    "class_code_pool_net = inspect.getsource(pool_net)\n",
    "with open(f'{hps.model_dir}/model_net.py.bak', 'w') as f:\n",
    "    f.write(\"import torch\\nimport torch.nn as nn\\n\\n\")\n",
    "    f.write(class_code_pool_net)\n",
    "\n",
    "# =============================================================\n",
    "# SECTION: Additional Setup\n",
    "# =============================================================\n",
    "# if hps.model.pooling_model.split(\"_\")[0] == \"WavLM\":\n",
    "#     print(\"Loaded Pretrained WavLM\")\n",
    "#     ssl_model = AutoModel.from_pretrained(\"microsoft/wavlm-large\")\n",
    "#     pool_model.feature_extractor.load_state_dict(ssl_model.feature_extractor.state_dict())\n",
    "#     pool_model.feature_extractor._freeze_parameters()\n",
    "#     del ssl_model\n",
    "# elif hps.model.pooling_model.split(\"_\")[0] == \"Whisper\":\n",
    "#     print(\"Loaded Pretrained Whisper\")\n",
    "#     ssl_model = AutoModel.from_pretrained(\"openai/whisper-large-v3\") \n",
    "#     pool_model.feature_extractor.conv1.load_state_dict(ssl_model.encoder.conv1.state_dict())\n",
    "#     pool_model.feature_extractor.conv2.load_state_dict(ssl_model.encoder.conv2.state_dict())\n",
    "#     #pool_model.feature_extractor.embed_positions.load_state_dict(ssl_model.encoder.embed_positions.state_dict())\n",
    "#     pool_model.feature_extractor._freeze_parameters()\n",
    "#     del ssl_model\n",
    "    \n",
    "# =============================================================\n",
    "# SECTION: Setup Logger, Dataloader\n",
    "# =============================================================\n",
    "epoch_str = 1\n",
    "global_step = 0\n",
    "\n",
    "pool_net = getattr(models, hps.model.pooling_model)\n",
    "pool_model = pool_net(hps.model.feature_dim, **hps.model).cuda()\n",
    "\n",
    "optimizer_p = torch.optim.AdamW(pool_model.parameters(), hps.train.learning_rate, betas=hps.train.betas, eps=hps.train.eps)\n",
    "scheduler_p = torch.optim.lr_scheduler.ExponentialLR(optimizer_p, gamma=hps.train.lr_decay, last_epoch=epoch_str - 2)\n",
    "\n",
    "_, _, _, _, epoch_str = utils.load_checkpoint(\n",
    "    os.path.join(hps.model_dir, \"best_pool.pth\"),\n",
    "    pool_model,\n",
    "    optimizer_p,\n",
    "    scheduler_p,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:10<00:00,  2.32it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.29296875"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool_model.eval() \n",
    "all_preds, all_labels, all_wavnames, all_embeddings  = [], [], [], []\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (wav_names, audio, attention_masks, dse_ids, spk_ids) in enumerate(tqdm(val_loader)):\n",
    "        audio = audio.cuda(non_blocking=True).float().squeeze(1)\n",
    "        attention_masks = attention_masks.cuda(non_blocking=True).float()\n",
    "        dse_ids = dse_ids.cuda(non_blocking=True).float()\n",
    "        spk_ids = spk_ids.cuda(non_blocking=True).long()\n",
    "\n",
    "        x_lengths = torch.tensor(commons.compute_length_from_mask(attention_masks)).cuda(non_blocking=True).long()\n",
    "        out_model = pool_model(audio)\n",
    "        outputs = out_model[0]\n",
    "        \n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        dse_ids = np.argmax(dse_ids.cpu().detach().numpy(), axis=-1)\n",
    "\n",
    "        all_wavnames.extend(wav_names)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        #all_embeddings.extend(out_model[1].cpu().numpy())\n",
    "        all_labels.extend(dse_ids)\n",
    "\n",
    "all_labels = np.array(all_labels)\n",
    "all_preds = np.array(all_preds)\n",
    "all_wavnames = np.array(all_wavnames)\n",
    "#all_embeddings = np.array(all_embeddings)\n",
    "\n",
    "accuracy_score(all_labels, all_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "import os, json, pickle, inspect\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.amp import GradScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import utils\n",
    "import commons\n",
    "import models\n",
    "from cough_datasets import CoughDatasets, CoughDatasetsCollate\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, confusion_matrix, roc_curve, auc, roc_auc_score, f1_score\n",
    "\n",
    "from tensorboard.backend.event_processing import event_accumulator\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "\n",
    "# =============================================================\n",
    "# SECTION: Intialize Data\n",
    "# =============================================================\n",
    "INIT = False\n",
    "MODEL_NAME = \"resnet_reproduce_cam\"\n",
    "CONFIG_PATH = \"configs/lstm_cnn.json\"\n",
    "\n",
    "model_dir = os.path.join(\"./logs\", MODEL_NAME)\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "\n",
    "config_save_path = os.path.join(model_dir, \"config.json\")\n",
    "if INIT:\n",
    "    with open(CONFIG_PATH, \"r\") as f:\n",
    "      data = f.read()\n",
    "    with open(config_save_path, \"w\") as f:\n",
    "      f.write(data)\n",
    "else:\n",
    "    with open(config_save_path, \"r\") as f:\n",
    "      data = f.read()\n",
    "\n",
    "config = json.loads(data)\n",
    "  \n",
    "hps = utils.HParams(**config)\n",
    "hps.model_dir = model_dir\n",
    "\n",
    "BATCH_SIZE = hps.train.batch_size\n",
    "ACCUMULATION_STEP = hps.train.accumulation_steps\n",
    "assert (ACCUMULATION_STEP > 0) and (BATCH_SIZE % ACCUMULATION_STEP == 0)\n",
    "cur_bs = BATCH_SIZE // ACCUMULATION_STEP\n",
    "\n",
    "# =============================================================\n",
    "# SECTION: Loading Data\n",
    "# =============================================================\n",
    "\n",
    "##### Label Umum Semua\n",
    "Diseases_codes = [0, 1]\n",
    "CLASS_NAMES = [\"Negative TB\", \"Positive TB\"]\n",
    "\n",
    "df = pd.read_csv(f'{hps.data.db_path}/GoogleHealth/google_tb_metadata.csv')\n",
    "df['path_file'] = f'GoogleHealth/' + df['path_file'] \n",
    "\n",
    "collate_fn = CoughDatasetsCollate(hps.data.many_class)\n",
    "val_dataset = CoughDatasets(df.values, hps.data, train=False)\n",
    "val_loader = DataLoader(val_dataset, num_workers=28, shuffle=False, batch_size=hps.train.batch_size, pin_memory=True, drop_last=True, collate_fn=collate_fn)\n",
    "\n",
    "# =============================================================\n",
    "# SECTION: Setup Logger, Dataloader\n",
    "# =============================================================\n",
    "epoch_str = 1\n",
    "global_step = 0\n",
    "\n",
    "pool_net = getattr(models, hps.model.pooling_model)\n",
    "pool_model = pool_net(hps.model.feature_dim, **hps.model).cuda()\n",
    "\n",
    "optimizer_p = torch.optim.AdamW(pool_model.parameters(), hps.train.learning_rate, betas=hps.train.betas, eps=hps.train.eps)\n",
    "scheduler_p = torch.optim.lr_scheduler.ExponentialLR(optimizer_p, gamma=hps.train.lr_decay, last_epoch=epoch_str - 2)\n",
    "\n",
    "_, _, _, _, epoch_str = utils.load_checkpoint(\n",
    "    os.path.join(hps.model_dir, \"best_pool.pth\"),\n",
    "    pool_model,\n",
    "    optimizer_p,\n",
    "    scheduler_p,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:01<00:00, 27.10it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3241421568627451"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool_model.eval() \n",
    "all_preds, all_labels, all_wavnames, all_embeddings  = [], [], [], []\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (wav_names, audio, attention_masks, dse_ids, spk_ids) in enumerate(tqdm(val_loader)):\n",
    "        audio = audio.cuda(non_blocking=True).float().squeeze(1)\n",
    "        attention_masks = attention_masks.cuda(non_blocking=True).float()\n",
    "        dse_ids = dse_ids.cuda(non_blocking=True).float()\n",
    "        spk_ids = spk_ids.cuda(non_blocking=True).long()\n",
    "\n",
    "        x_lengths = torch.tensor(commons.compute_length_from_mask(attention_masks)).cuda(non_blocking=True).long()\n",
    "        out_model = pool_model(audio)\n",
    "        outputs = out_model[0]\n",
    "        \n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        dse_ids = np.argmax(dse_ids.cpu().detach().numpy(), axis=-1)\n",
    "\n",
    "        all_wavnames.extend(wav_names)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        #all_embeddings.extend(out_model[1].cpu().numpy())\n",
    "        all_labels.extend(dse_ids)\n",
    "\n",
    "all_labels = np.array(all_labels)\n",
    "all_preds = np.array(all_preds)\n",
    "all_wavnames = np.array(all_wavnames)\n",
    "#all_embeddings = np.array(all_embeddings)\n",
    "\n",
    "accuracy_score(all_labels, all_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(656292, 5)\n",
      "tensor([1.2743, 0.8229], device='cuda:0')\n",
      "True\n",
      "False\n",
      "(80, 63)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "df = pd.read_csv(f'{hps.data.db_path}/{hps.data.metadata_csv}')\n",
    "df = df[df['cough_score'] >= 0.90].sample(frac=1, random_state=40)\n",
    "\n",
    "df_solic = df[df['type_cough'] == 0].sample(frac=1, random_state=41)\n",
    "df_long = df[df['type_cough'] == 1].sample(frac=1, random_state=42) # 0 Solic, 1 Longi\n",
    "df_long_array = []\n",
    "for i_rand in range(5):\n",
    "    df_0 = df_long[df_long['disease_label'] == 0].sample(n=df_solic['disease_label'].value_counts()[0], random_state=i_rand * 4)\n",
    "    df_1 = df_long[df_long['disease_label'] == 1].sample(n=df_solic['disease_label'].value_counts()[1], random_state=i_rand * 4)\n",
    "    df_long_array.append(pd.concat([df_0, df_1], ignore_index=True, sort=False))\n",
    "\n",
    "df = df\n",
    "print(df.shape)\n",
    "\n",
    "df_train, df_test = train_test_split(df, test_size=0.1, random_state=42, shuffle=True)\n",
    "df_issue = pd.read_csv(\"df_issue.csv\")\n",
    "df_train = df_train[~df_train['path_file'].isin(df_issue['wavname'])]\n",
    "df_test = df_test[~df_test['path_file'].isin(df_issue['wavname'])]\n",
    "\n",
    "class_frequencies = df_train['disease_label'].value_counts().to_dict()\n",
    "total_samples = len(df_train)\n",
    "class_weights = {cls: total_samples / (len(Diseases_codes) * freq) if freq != 0 else 0 for cls, freq in class_frequencies.items()}\n",
    "weights_list = [class_weights[cls] for cls in Diseases_codes]\n",
    "class_weights_tensor = torch.tensor(weights_list, device='cuda', dtype=torch.float)\n",
    "print(class_weights_tensor)\n",
    "\n",
    "# =============================================================\n",
    "# SECTION: Setup Logger, Dataloader\n",
    "# =============================================================\n",
    "writer = SummaryWriter(log_dir=hps.model_dir)\n",
    "writer_eval = SummaryWriter(log_dir=os.path.join(hps.model_dir, \"eval\"))\n",
    "\n",
    "collate_fn = CoughDatasetsCollate(hps.data.many_class)\n",
    "train_dataset = CoughDatasets(df_train.values, hps.data, train=True)\n",
    "val_dataset = CoughDatasets(df_test.values, hps.data, train=False)\n",
    "\n",
    "#train_sampler = DistributedBucketSampler(train_dataset, cur_bs, [32,300,400,500,600,700,800,900,1000], num_replicas=1, rank=0, shuffle=True)\n",
    "#train_loader = DataLoader(train_dataset, num_workers=28, shuffle=False, pin_memory=True, collate_fn=collate_fn, batch_sampler=train_sampler)\n",
    "train_loader = DataLoader(train_dataset, num_workers=28, shuffle=True, batch_size=cur_bs, pin_memory=True, drop_last=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, num_workers=28, shuffle=False, batch_size=hps.train.batch_size, pin_memory=True, drop_last=True, collate_fn=collate_fn)\n",
    "\n",
    "print(next(iter(train_loader))[1][0].numpy().shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f'{hps.data.db_path}/{hps.data.metadata_csv}')\n",
    "df_solic = df[df['type_cough'] == 0].sample(frac=1, random_state=41)\n",
    "df_long = df[df['type_cough'] == 1].sample(frac=1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "647060"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_long['disease_label'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "disease_label\n",
       "1    396182\n",
       "0    250878\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_long['disease_label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = df[df['cough_score'] >= 0.90].sample(frac=1, random_state=40)\n",
    "\n",
    " # 0 Solic, 1 Longi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9228/9228 [00:44<00:00, 206.97it/s]\n",
      "100%|██████████| 512/512 [00:09<00:00, 54.26it/s] \n"
     ]
    }
   ],
   "source": [
    "pool_model.eval() \n",
    "all_preds, all_labels, all_wavnames, all_embeddings  = [], [], [], []\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (wav_names, audio, attention_masks, dse_ids, spk_ids) in enumerate(tqdm(train_loader)):\n",
    "        audio = audio.cuda(non_blocking=True).float().squeeze(1)\n",
    "        attention_masks = attention_masks.cuda(non_blocking=True).float()\n",
    "        dse_ids = dse_ids.cuda(non_blocking=True).float()\n",
    "        spk_ids = spk_ids.cuda(non_blocking=True).long()\n",
    "\n",
    "        x_lengths = torch.tensor(commons.compute_length_from_mask(attention_masks)).cuda(non_blocking=True).long()\n",
    "        out_model = pool_model(audio)\n",
    "        outputs = out_model[0]\n",
    "        \n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        dse_ids = np.argmax(dse_ids.cpu().detach().numpy(), axis=-1)\n",
    "\n",
    "        all_wavnames.extend(wav_names)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_embeddings.extend(out_model[1].cpu().numpy())\n",
    "        all_labels.extend(dse_ids)\n",
    "\n",
    "    for batch_idx, (wav_names, audio, attention_masks, dse_ids, spk_ids) in enumerate(tqdm(val_loader)):\n",
    "        audio = audio.cuda(non_blocking=True).float().squeeze(1)\n",
    "        attention_masks = attention_masks.cuda(non_blocking=True).float()\n",
    "        dse_ids = dse_ids.cuda(non_blocking=True).float()\n",
    "        spk_ids = spk_ids.cuda(non_blocking=True).long()\n",
    "\n",
    "        x_lengths = torch.tensor(commons.compute_length_from_mask(attention_masks)).cuda(non_blocking=True).long()\n",
    "        out_model = pool_model(audio)\n",
    "        outputs = out_model[0]\n",
    "        \n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        dse_ids = np.argmax(dse_ids.cpu().detach().numpy(), axis=-1)\n",
    "\n",
    "        all_wavnames.extend(wav_names)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_embeddings.extend(out_model[1].cpu().numpy())\n",
    "        all_labels.extend(dse_ids)\n",
    "\n",
    "all_labels = np.array(all_labels)\n",
    "all_preds = np.array(all_preds)\n",
    "all_wavnames = np.array(all_wavnames)\n",
    "all_embeddings = np.array(all_embeddings)\n",
    "\n",
    "df_result = pd.DataFrame({\n",
    "    'wavname': all_wavnames,\n",
    "    'label': all_labels,\n",
    "    'pred': all_preds\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9636097361747781"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(all_labels, all_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mismatch = df_result[df_result['label'] != df_result['pred']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mismatch.to_csv(\"mismatch_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result['wavname'] = hps.data.db_path + df_result['wavname']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-07-07 12:01:56.809\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mrenumics.spotlight.analysis.analyzers.cleanlab\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m20\u001b[0m - \u001b[33m\u001b[1mCleanlab analyzer requires `cleanlab` to be installed.\u001b[0m\n",
      "\u001b[32m2025-07-07 12:01:56.810\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mrenumics.spotlight.analysis.analyzers.cleanvision\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m24\u001b[0m - \u001b[33m\u001b[1mCleanvision analyzer requires `cleanvision` to be installed.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using precomputed embeddings.\n",
      "Pre-reducing feature wavname in mode native.\n",
      "Using op mix ratio 0.8.\n",
      "Using num dimensions 32.\n",
      "The overall metric value is 0.9636168552477565\n",
      "You didn't specify metric_mode parameter. Using max as default.\n",
      "Detecting issues for criteria n_slices=20, criterion=drop, min_drop=None, min_support=None.\n",
      "Identified 20 problematic slices.\n"
     ]
    }
   ],
   "source": [
    "from sliceguard import SliceGuard\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "sg = SliceGuard()\n",
    "issues = sg.find_issues(df_result, features=[\"wavname\"], y=\"label\", y_pred=\"pred\", metric=accuracy_score, precomputed_embeddings={\"wavname\": all_embeddings})\n",
    "report_df, spotlight_data_issues, spotlight_dtypes, spotlight_layout = sg.report(no_browser=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "469a87e313434a6a8e57ed19520517cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Spotlight running on http://127.0.0.1:37719/'), HBox(children=(Button(description=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from renumics import spotlight\n",
    "\n",
    "spotlight.show(df_result.reset_index(), issues=spotlight_data_issues, layout=spotlight_layout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_selected_rows = []\n",
    "\n",
    "for now_data_issue in spotlight_data_issues:\n",
    "    selected_rows = df_result.iloc[now_data_issue.rows]\n",
    "    combined_selected_rows.append(selected_rows)\n",
    "\n",
    "# Concatenate all selected rows into a single DataFrame\n",
    "final_selected_df = pd.concat(combined_selected_rows, ignore_index=True)\n",
    "final_selected_df['wavname'] = final_selected_df['wavname'].str.replace(hps.data.db_path, \"\", regex=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wavname</th>\n",
       "      <th>label</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CombineData2/TB/longitudinal_data/166031369478...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CombineData2/TB/longitudinal_data/163971760433...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CombineData2/TB/longitudinal_data/163958681315...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CombineData2/TB/longitudinal_data/163308643539...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CombineData2/TB/longitudinal_data/164450756765...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CombineData2/TB/longitudinal_data/163446334743...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CombineData2/TB/longitudinal_data/165236057349...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>CombineData2/TB/longitudinal_data/164475324249...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>CombineData2/TB/longitudinal_data/165225839680...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>CombineData2/TB/longitudinal_data/164165415633...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>CombineData2/TB/longitudinal_data/164294984080...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>CombineData2/TB/longitudinal_data/163776240865...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>CombineData2/TB/longitudinal_data/164691959683...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>CombineData2/TB/longitudinal_data/164511965591...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>CombineData2/TB/longitudinal_data/165439486028...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>CombineData2/TB/longitudinal_data/163662773619...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>CombineData2/TB/longitudinal_data/164762003750...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>CombineData2/TB/longitudinal_data/165376275708...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>CombineData2/TB/longitudinal_data/163390241327...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>CombineData2/TB/longitudinal_data/164504224676...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>CombineData2/TB/longitudinal_data/164242137476...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>CombineData2/TB/longitudinal_data/164057785468...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>CombineData2/TB/longitudinal_data/164258718297...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>CombineData2/TB/longitudinal_data/164309346065...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>CombineData2/TB/longitudinal_data/164916448178...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>CombineData2/TB/longitudinal_data/164325244240...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>CombineData2/TB/longitudinal_data/165515799525...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>CombineData2/TB/longitudinal_data/162474336252...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>CombineData2/TB/longitudinal_data/163539653768...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>CombineData2/TB/longitudinal_data/162341803102...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>CombineData2/TB/longitudinal_data/164369838567...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>CombineData2/TB/longitudinal_data/164933111260...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>CombineData2/TB/solicited_data/1653358366289-r...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>CombineData2/TB/longitudinal_data/163463744124...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>CombineData2/TB/longitudinal_data/165338233013...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>CombineData2/TB/longitudinal_data/165227108072...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>CombineData2/TB/longitudinal_data/164734137939...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>CombineData2/TB/longitudinal_data/164257402534...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>CombineData2/TB/longitudinal_data/164982184380...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>CombineData2/TB/longitudinal_data/164407519123...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>CombineData2/TB/longitudinal_data/163417303522...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>CombineData2/TB/longitudinal_data/164528931303...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>CombineData2/TB/longitudinal_data/163758124345...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>CombineData2/TB/longitudinal_data/165621998211...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>CombineData2/TB/longitudinal_data/163484244273...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>CombineData2/TB/longitudinal_data/162913451401...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>CombineData2/TB/longitudinal_data/165356961303...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              wavname  label  pred\n",
       "0   CombineData2/TB/longitudinal_data/166031369478...      1     0\n",
       "1   CombineData2/TB/longitudinal_data/163971760433...      1     0\n",
       "2   CombineData2/TB/longitudinal_data/163958681315...      1     0\n",
       "3   CombineData2/TB/longitudinal_data/163308643539...      0     1\n",
       "4   CombineData2/TB/longitudinal_data/164450756765...      0     1\n",
       "5   CombineData2/TB/longitudinal_data/163446334743...      0     1\n",
       "6   CombineData2/TB/longitudinal_data/165236057349...      0     1\n",
       "7   CombineData2/TB/longitudinal_data/164475324249...      0     1\n",
       "8   CombineData2/TB/longitudinal_data/165225839680...      0     1\n",
       "9   CombineData2/TB/longitudinal_data/164165415633...      1     0\n",
       "10  CombineData2/TB/longitudinal_data/164294984080...      1     0\n",
       "11  CombineData2/TB/longitudinal_data/163776240865...      1     0\n",
       "12  CombineData2/TB/longitudinal_data/164691959683...      1     0\n",
       "13  CombineData2/TB/longitudinal_data/164511965591...      0     1\n",
       "14  CombineData2/TB/longitudinal_data/165439486028...      1     0\n",
       "15  CombineData2/TB/longitudinal_data/163662773619...      0     1\n",
       "16  CombineData2/TB/longitudinal_data/164762003750...      0     1\n",
       "17  CombineData2/TB/longitudinal_data/165376275708...      0     1\n",
       "18  CombineData2/TB/longitudinal_data/163390241327...      0     1\n",
       "19  CombineData2/TB/longitudinal_data/164504224676...      0     1\n",
       "20  CombineData2/TB/longitudinal_data/164242137476...      1     0\n",
       "21  CombineData2/TB/longitudinal_data/164057785468...      0     1\n",
       "22  CombineData2/TB/longitudinal_data/164258718297...      0     1\n",
       "23  CombineData2/TB/longitudinal_data/164309346065...      1     0\n",
       "24  CombineData2/TB/longitudinal_data/164916448178...      1     0\n",
       "25  CombineData2/TB/longitudinal_data/164325244240...      1     0\n",
       "26  CombineData2/TB/longitudinal_data/165515799525...      0     1\n",
       "27  CombineData2/TB/longitudinal_data/162474336252...      0     1\n",
       "28  CombineData2/TB/longitudinal_data/163539653768...      0     1\n",
       "29  CombineData2/TB/longitudinal_data/162341803102...      0     1\n",
       "30  CombineData2/TB/longitudinal_data/164369838567...      0     1\n",
       "31  CombineData2/TB/longitudinal_data/164933111260...      0     1\n",
       "32  CombineData2/TB/solicited_data/1653358366289-r...      1     0\n",
       "33  CombineData2/TB/longitudinal_data/163463744124...      1     0\n",
       "34  CombineData2/TB/longitudinal_data/165338233013...      0     1\n",
       "35  CombineData2/TB/longitudinal_data/165227108072...      0     1\n",
       "36  CombineData2/TB/longitudinal_data/164734137939...      0     1\n",
       "37  CombineData2/TB/longitudinal_data/164257402534...      0     1\n",
       "38  CombineData2/TB/longitudinal_data/164982184380...      0     1\n",
       "39  CombineData2/TB/longitudinal_data/164407519123...      1     0\n",
       "40  CombineData2/TB/longitudinal_data/163417303522...      0     1\n",
       "41  CombineData2/TB/longitudinal_data/164528931303...      0     1\n",
       "42  CombineData2/TB/longitudinal_data/163758124345...      1     0\n",
       "43  CombineData2/TB/longitudinal_data/165621998211...      0     1\n",
       "44  CombineData2/TB/longitudinal_data/163484244273...      0     1\n",
       "45  CombineData2/TB/longitudinal_data/162913451401...      1     0\n",
       "46  CombineData2/TB/longitudinal_data/165356961303...      1     0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_selected_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_selected_df.to_csv(\"df_issue2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_result = df_result[~df_result['wavname'].isin(final_selected_df['wavname'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(656292, 5)\n"
     ]
    }
   ],
   "source": [
    "import os, json, pickle, inspect\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.amp import GradScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoModel, AutoConfig\n",
    "\n",
    "import utils\n",
    "import commons\n",
    "import models\n",
    "from cough_datasets import CoughDatasets, CoughDatasetsCollate\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, confusion_matrix, roc_curve, auc, roc_auc_score, f1_score\n",
    "\n",
    "from tensorboard.backend.event_processing import event_accumulator\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "\n",
    "# =============================================================\n",
    "# SECTION: Intialize Data\n",
    "# =============================================================\n",
    "INIT = False\n",
    "MODEL_NAME = \"try_lstm_sken3_exludedf_issue\"\n",
    "CONFIG_PATH = \"configs/lstm_cnn.json\"\n",
    "\n",
    "model_dir = os.path.join(\"./logs\", MODEL_NAME)\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "\n",
    "config_save_path = os.path.join(model_dir, \"config.json\")\n",
    "if INIT:\n",
    "    with open(CONFIG_PATH, \"r\") as f:\n",
    "      data = f.read()\n",
    "    with open(config_save_path, \"w\") as f:\n",
    "      f.write(data)\n",
    "else:\n",
    "    with open(config_save_path, \"r\") as f:\n",
    "      data = f.read()\n",
    "\n",
    "config = json.loads(data)\n",
    "  \n",
    "hps = utils.HParams(**config)\n",
    "hps.model_dir = model_dir\n",
    "\n",
    "BATCH_SIZE = hps.train.batch_size\n",
    "ACCUMULATION_STEP = hps.train.accumulation_steps\n",
    "assert (ACCUMULATION_STEP > 0) and (BATCH_SIZE % ACCUMULATION_STEP == 0)\n",
    "cur_bs = BATCH_SIZE // ACCUMULATION_STEP\n",
    "\n",
    "# =============================================================\n",
    "# SECTION: Loading Data\n",
    "# =============================================================\n",
    "\n",
    "##### Label Umum Semua\n",
    "Diseases_codes = [0, 1]\n",
    "CLASS_NAMES = [\"Healthy\", \"TB\"]\n",
    "\n",
    "df = pd.read_csv(f'{hps.data.db_path}/{hps.data.metadata_csv}')\n",
    "df = df[df['cough_score'] >= 0.90].sample(frac=1, random_state=40)\n",
    "\n",
    "df_solic = df[df['type_cough'] == 0].sample(frac=1, random_state=41)\n",
    "df_long = df[df['type_cough'] == 1].sample(frac=1, random_state=42) # 0 Solic, 1 Longi\n",
    "df_long_array = []\n",
    "for i_rand in range(5):\n",
    "    df_0 = df_long[df_long['disease_label'] == 0].sample(n=df_solic['disease_label'].value_counts()[0], random_state=i_rand * 4)\n",
    "    df_1 = df_long[df_long['disease_label'] == 1].sample(n=df_solic['disease_label'].value_counts()[1], random_state=i_rand * 4)\n",
    "    df_long_array.append(pd.concat([df_0, df_1], ignore_index=True, sort=False))\n",
    "\n",
    "df = df\n",
    "#df = df_solic\n",
    "#df = df_long_array[0]\n",
    "#df = df_long\n",
    "print(df.shape)\n",
    "\n",
    "df_train, df_test = train_test_split(df, test_size=0.1, random_state=42, shuffle=True)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
